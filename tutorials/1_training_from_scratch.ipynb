{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Mamba-based segmentation model from scratch\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "\n",
    "- If you want to run this notebook you will only need the AISHELL dataset (or any dataset ready as a `pyannote.database` protocol). Such protocol preparation scripts are available at https://github.com/FrenchKrab/datasets-pyannote.\n",
    "- You will need a valid plaqntt installation, please check the previous notebook if you have trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying an architecture\n",
    "\n",
    "Architectures are specified with a dictionary by specifying the `blocks` parameter of the `SdResBlocks` class. \n",
    "\n",
    "For convenience we provide a list of premade architectures in [architectures/blocks/](./architectures/blocks/), including the configuration used in the paper : [mamba_plaqntt.yaml](./architectures/blocks/mamba_plaqntt.yaml).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the architecture file\n",
    "with open('architectures/blocks/mamba_plaqntt.yaml', 'r') as file:\n",
    "    blocks_architecture = yaml.safe_load(file)\n",
    "blocks_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pyannote database protocol of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import registry, FileFinder\n",
    "\n",
    "DB_YAML_PATH = '/home/aplaquet/work58/databases/database.yml'\n",
    "PROTOCOL_NAME = 'AISHELL4.SpeakerDiarization.Custom'\n",
    "\n",
    "registry.load_database(DB_YAML_PATH)\n",
    "protocol = registry.get_protocol(PROTOCOL_NAME, preprocessors={\"audio\": FileFinder()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the task. This will influence the model architecture and loss function (it sets the number of features/speakers in the output tensor, and whether to use powerset cross entropy loss or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks.segmentation.speaker_diarization import SpeakerDiarization\n",
    "\n",
    "# Create a small model for this tutorial, but you can increase the duration and max number of speakers to match the paper configuration\n",
    "task = SpeakerDiarization(\n",
    "    protocol=protocol,\n",
    "    duration=5.0,\n",
    "    max_speakers_per_chunk=4,\n",
    "    max_speakers_per_frame=None,    # use only for low duration models\n",
    "    batch_size=32,\n",
    "    balance=['database'],   # useless here, but useful for compound datasets\n",
    "    num_workers=1,  # set to number of cores\n",
    "    cache=f'{PROTOCOL_NAME}.protocol.cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You will need a recent GPU to initialize this model, the architecture building calls model inference (to estimate parameter count), and the current implementation of Mamba only runs on CUDA for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plaqntt.pyannote_audio.sdresblocks import SdResBlocks\n",
    "import torch\n",
    "\n",
    "mamba_diar = SdResBlocks(\n",
    "    wav2vec='WAVLM_BASE_PLUS',\n",
    "    wav2vec_layer=-1,\n",
    "    blocks=blocks_architecture,\n",
    "    linear={\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "    },\n",
    "    task=task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.prepare_data() # preload the protocol and save the cache to disk \n",
    "task.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the trainer. Here we only train for one epoch but feel free to change things as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator='gpu',\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "trainer.validate(mamba_diar)\n",
    "trainer.fit(mamba_diar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
